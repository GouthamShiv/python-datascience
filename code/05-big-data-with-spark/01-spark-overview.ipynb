{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](./00-index.ipynb)\n",
    "\n",
    "---\n",
    "## `Apache Spark - Overview`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `What is Spark?`\n",
    "\n",
    "- **Spark** is a framework for distributed framework\n",
    "- It is a streamlined alternative to Map-Reduce\n",
    "- **Spark** applications can be written in Scala, Java or Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Why Spark?`\n",
    "\n",
    "**Why learn Spark**\n",
    "\n",
    "- **Spark** enables you to analyze petabytes of data\n",
    "- **Spark** is significantly faster than Map-Reduce\n",
    "- Paradoxically, **Spark's** API is simpler than the Map-Reduce API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Origins`\n",
    "\n",
    "- **Spark** was initially started at **UC Berkeley's AMPLab** (AMP = Algorithms Machines People) in 2009\n",
    "- After being open sourced in 2010 under a BSD license, the project was donated in 2013 to the _Apache Software Foundation_ and switched its license to Apache 2.0.\n",
    "- **Spark** is one of the most active projects in the _Apache Software Foundation_ and one of the most active open source big data projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Essence of Spark`\n",
    "\n",
    "What is the basic idea of **Spark**?\n",
    "\n",
    "- **Spark** takes the Map-Reduce paradigm and changes it in some critical ways:\n",
    "  - Instead of writing single Map-Reduce jobs, a **Spark** job consists of a series of map and reduce functions\n",
    "  - Moreover, the intermediate data is kept in memory instead of being written to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Spark Ecosystem`\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../../assets/spark_ecosystem.png\" alt=\"Spark Ecosystem\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Spark Logging`\n",
    "\n",
    "Q: How can I make Spark logging less verbose?\n",
    "\n",
    "- By default Spark logs messages at the **INFO** level\n",
    "- Here are the steps to make it only print out warnings and errors\n",
    "    ```bash\n",
    "    cd $SPARK_HOME/conf\n",
    "    cp log4j.properties.template log4j.properties\n",
    "    ```\n",
    "- Edit `log4j.properties` and replace `rootCategory=INFO` with `rootCategory=ERROR`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Spark Execution`\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../../assets/spark_execution.png\" alt=\"Spark Execution\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Spark Terminology`\n",
    "\n",
    "|Term|Meaning|\n",
    "|----:|----:|\n",
    "|Driver|Process that contains the Spark Context|\n",
    "|Executor|Process that executes one or more Spark tasks|\n",
    "|Master|Process which manages applications across the cluster, e.g., Spark Master|\n",
    "|Worker|Process which manages executors on a particular worker node, e.g., Spark Worker|\n",
    "\n",
    "\n",
    "<!-- <p align=\"center\">\n",
    "  <img src=\"../../assets/spark_terminology.png\" alt=\"Spark Terminology\">\n",
    "</p> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Spark Job`\n",
    "\n",
    "Q: Flip a coin 100 times using Python's `random()` function. What fraction of the time do you get heads?\n",
    "\n",
    "- Initialize Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/goutham/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/06/12 01:37:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "spark = ps.sql.SparkSession.builder.master('local[*]').appName('spark-lecture').getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define and run the Spark job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heads:  55\n",
      "tails:  45\n",
      "ratio:  0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "n = 100\n",
    "\n",
    "heads = (sc.parallelize(range(n))\n",
    "        .map(lambda _: random.random())\n",
    "        .filter(lambda r: r < 0.5).count())\n",
    "\n",
    "tails = n - heads\n",
    "ratio = 1. * heads / n\n",
    "\n",
    "print('heads: ', heads)\n",
    "print('tails: ', tails)\n",
    "print('ratio: ', ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Notes`\n",
    "\n",
    "- `sc.parallelize` creates a **RDD**\n",
    "- **map** and **filter** are _transformations_\n",
    "  - They create new RDDs from existing RDDs\n",
    "- `count` is an _action_ and brings the data from the RDDs back to the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Spark Terminology`\n",
    "\n",
    "|Term|Meaning|\n",
    "|----:|----:|\n",
    "|RDD|_Resilient Distributed Dataset_ or distributed sequence of records|\n",
    "|Spark Job|Sequence of transformations on data with a final action|\n",
    "|Transformation|Spark operation that produces a local object|\n",
    "|Action|Spark operation that produces a local object|\n",
    "|Spark Application|Sequence of Spark jobs and other code|\n",
    "\n",
    "- A Spark job pushes the data to the cluster, all computation happens on the _executors_, then the result is sent back to the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Understanding The Code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(n))\n",
    "rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11048732820468354,\n",
       " 0.2520917913435715,\n",
       " 0.8319029660855509,\n",
       " 0.2914023400457618,\n",
       " 0.8970176817649934,\n",
       " 0.5486326308361258,\n",
       " 0.7385884898295728,\n",
       " 0.26980100052010847,\n",
       " 0.3145232246993669,\n",
       " 0.3956785313451111,\n",
       " 0.9803245403190293,\n",
       " 0.14465830445192274,\n",
       " 0.6200910604134594,\n",
       " 0.4066266127638466,\n",
       " 0.24113503509411793,\n",
       " 0.5454709067324557,\n",
       " 0.4978431140223679,\n",
       " 0.2502245803915162,\n",
       " 0.8594570615813439,\n",
       " 0.8234675095828748,\n",
       " 0.7050156811493302,\n",
       " 0.030337138062836222,\n",
       " 0.8546739968794432,\n",
       " 0.664557897079867,\n",
       " 0.969961992753474,\n",
       " 0.8461086209431787,\n",
       " 0.368085476859066,\n",
       " 0.46083364596089527,\n",
       " 0.15768102554919072,\n",
       " 0.9564927899169007,\n",
       " 0.030701485035367626,\n",
       " 0.009775122850377294,\n",
       " 0.0014013184543518742,\n",
       " 0.1809761358875862,\n",
       " 0.05154358750738908,\n",
       " 0.27889969724750985,\n",
       " 0.45189814272317075,\n",
       " 0.8033567360812239,\n",
       " 0.8433865912910039,\n",
       " 0.09128490953951807,\n",
       " 0.19739073665553974,\n",
       " 0.6809153907454448,\n",
       " 0.29537337987693113,\n",
       " 0.9618046008485446,\n",
       " 0.9630071657997124,\n",
       " 0.36541703333440034,\n",
       " 0.05672970524670384,\n",
       " 0.9647536333026684,\n",
       " 0.49085656306639125,\n",
       " 0.5643886668671354,\n",
       " 0.4204637913957958,\n",
       " 0.5223739209505103,\n",
       " 0.3509765702029879,\n",
       " 0.9454331289661588,\n",
       " 0.6203640742971037,\n",
       " 0.22322898781627576,\n",
       " 0.6220604279054258,\n",
       " 0.9153935533608236,\n",
       " 0.6860262683829477,\n",
       " 0.08821908034263759,\n",
       " 0.6733363227279039,\n",
       " 0.500204498709657,\n",
       " 0.6794388664789087,\n",
       " 0.48830603510225945,\n",
       " 0.7138878030693829,\n",
       " 0.05718237566090956,\n",
       " 0.12819147825891963,\n",
       " 0.5678747490050342,\n",
       " 0.8627395492588039,\n",
       " 0.3992009490851919,\n",
       " 0.8994849432212849,\n",
       " 0.17960236989997058,\n",
       " 0.2604348044413334,\n",
       " 0.9549233834038277,\n",
       " 0.6974898017926942,\n",
       " 0.6659449309592185,\n",
       " 0.09148978707231059,\n",
       " 0.4194631938937572,\n",
       " 0.766604007420933,\n",
       " 0.47535359531691457,\n",
       " 0.5710449952818216,\n",
       " 0.11654170202930281,\n",
       " 0.5959723721192539,\n",
       " 0.5934340410725362,\n",
       " 0.8783438648848902,\n",
       " 0.4450485534428309,\n",
       " 0.672200076563676,\n",
       " 0.8944291132672599,\n",
       " 0.4882015347802844,\n",
       " 0.530938601904382,\n",
       " 0.8057614466264404,\n",
       " 0.15802936043459193,\n",
       " 0.5172309181648952,\n",
       " 0.3420122105237102,\n",
       " 0.9489779833470556,\n",
       " 0.7750214731760867,\n",
       " 0.8409361807091639,\n",
       " 0.521328756374185,\n",
       " 0.33734500728322625,\n",
       " 0.17918971114012994]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rdd_map = rdd.map(lambda _: random.random())\n",
    "rdd_map.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24816898680350252,\n",
       " 0.4444352084761919,\n",
       " 0.46082730666770844,\n",
       " 0.15626997426544742,\n",
       " 0.18839250524454165,\n",
       " 0.23875664594081547,\n",
       " 0.14675140966361788,\n",
       " 0.15818163471476143,\n",
       " 0.2745584643078963,\n",
       " 0.28451161276227,\n",
       " 0.3316871503633313,\n",
       " 0.3488543892994437,\n",
       " 0.3596398365865848,\n",
       " 0.4802019564869422,\n",
       " 0.309853301377091,\n",
       " 0.31080375575390795,\n",
       " 0.3709706314232716,\n",
       " 0.38100470142811016,\n",
       " 0.4813917948410664,\n",
       " 0.007544145062662655,\n",
       " 0.3413447289221131,\n",
       " 0.2460790209455641,\n",
       " 0.29526142614240436,\n",
       " 0.2161198542410867,\n",
       " 0.35938939675019643,\n",
       " 0.38798432231761815,\n",
       " 0.24162704607217234,\n",
       " 0.46768288782966216,\n",
       " 0.0418106428657814,\n",
       " 0.30891176132171605,\n",
       " 0.42259389956982796,\n",
       " 0.29919169398132217,\n",
       " 0.4550719168768034,\n",
       " 0.41196462546289914,\n",
       " 0.26912474025176014,\n",
       " 0.4045402392603109,\n",
       " 0.3844404122021694,\n",
       " 0.2935035365436519,\n",
       " 0.01010808363407878,\n",
       " 0.15882162686221912,\n",
       " 0.19664329876067288,\n",
       " 0.059581051543504904,\n",
       " 0.2215007220660341,\n",
       " 0.0029917410447303228,\n",
       " 0.050915093828940994,\n",
       " 0.26563414330185553,\n",
       " 0.04206072887455725,\n",
       " 0.4392144732875747,\n",
       " 0.07371126966866393]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_filter = rdd_map.filter(lambda r: r < 0.5)\n",
    "rdd_filter.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_filter.count() # This is the action, to collect / count / mean etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Pop Quiz`\n",
    "\n",
    "In the below Spark job, what is the transformation? and what is the action?\n",
    "\n",
    "```python\n",
    "sc.parallelize(range(10)).filter(lambda x: x % 2 == 0).collect()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Lambda vs. Functions`\n",
    "\n",
    "- Instead of `lambda` we can pass in fully defined functions into `map`, `filter`, and other `RDD` transformations.\n",
    "- We often use `lambda` for short functions.\n",
    "- And, we use `def` for more substantial functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Another Example - Finding Primes`\n",
    "\n",
    "Q: Find all the primes less than 100.\n",
    "\n",
    "- Define function to determine if a number is prime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(number):\n",
    "  factor_min = 2\n",
    "  factor_max = int(number ** 0.5) + 1\n",
    "  for factor in range(factor_min, factor_max):\n",
    "    if number % factor == 0:\n",
    "      return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we use this function to filter non-primes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n"
     ]
    }
   ],
   "source": [
    "numbers = range(2, 100)\n",
    "\n",
    "primes = (sc.parallelize(numbers)\n",
    "          .filter(is_prime)\n",
    "          .collect())\n",
    "\n",
    "print(primes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Pop Quiz`\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../../assets/spark_execution.png\" alt=\"Spark Execution\">\n",
    "</p>\n",
    "\n",
    "Q: Where does `is_prime` execute?\n",
    "Q: Where does the RDD object get collected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Transformations and Actions`\n",
    "\n",
    "- Common RDD Constructs\n",
    "\n",
    "    |Expression|Meaning|\n",
    "    |:----:|:----|\n",
    "    |`sc.parallelize(list)`|Create RDD of elements of list|\n",
    "    |`sc.textFile(path)`|Create RDD of lines from file|\n",
    "<br>\n",
    "\n",
    "- Common Transformations\n",
    "\n",
    "    |Expression|Meaning|\n",
    "    |:----:|:----|\n",
    "    |`filter(lambda x: x % 2 == 0)`|Discard non-even elements|\n",
    "    |`map(lambda x: x * 2)`|Multiply each RDD element by 2|\n",
    "    |`map(lambda x: x.split())`|Split each string into words|\n",
    "    |`flatMap(lambda x: x.split())`|Split each string into words and flatten sequence|\n",
    "    |`sample(withReplacement = True, 0.25)`|Create sample of 25% of elements with replacement|\n",
    "    |`union(rdd)`|Append `rdd` to existing RDD|\n",
    "    |`distinct()`|Remove duplicates in RDD|\n",
    "    |`sortBy(lambda x: x, ascending = False)`|Sort elements in descending order|\n",
    "<br>\n",
    "\n",
    "- Common Actions\n",
    "\n",
    "    |Expression|Meaning|\n",
    "    |:----:|:----|\n",
    "    |`collect()`|Converts RDD to in-memory list|\n",
    "    |`take(3)`|First 3 elements of RDD|\n",
    "    |`top(3)`|Top 3 elements of RDD|\n",
    "    |`takeSample(withReplacement = True, 3)`|Create sample of 3 elements with replacement|\n",
    "    |`sum()`|Find element sum (assumes numeric elements)|\n",
    "    |`mean()`|Find element mean (assumes numeric elements)|\n",
    "    |`stdev()`|Find element deviation (assumes numeric elements)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Conclusion`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "[next]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b4484ae063bf9ef0d5115063857dd0e382a2c73ed49a11a000ca56af03302b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
