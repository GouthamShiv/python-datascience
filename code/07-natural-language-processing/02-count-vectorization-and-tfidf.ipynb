{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](./01-nlp-data-cleaning.ipynb)\n",
    "\n",
    "---\n",
    "## `Count Vectorization and TFIDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Count Vectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary:\n",
    "\n",
    "```python\n",
    "['drop', 'help', 'jeff', 'octopus', 'polic', 'said', 'sandwich', 'sandwichless', 'sobbed', 'stole']\n",
    "```\n",
    "\n",
    "Ideally, we want to assign simple values to words. So continuing with the data from previous section, we have these words organized as a list and one word is a *token*.  \n",
    "So, count vectorization is called a *bag-of-words*.  \n",
    "Meaning, we would want to count the frequency of the words appear in our document.\n",
    "\n",
    "In the previous section, we actually got lists of list and now we can convert it into numerical list as opposed to list with tokens.  \n",
    "And below is an example how the numerical list would look like for the example we had in the previous section.  \n",
    "\n",
    "```python\n",
    "['jeff', 'stole', 'octopus', 'sandwich']\n",
    "# [0, 0, 1, 1, 0, 0, 1, 0, 0, 1]\n",
    "['help', 'sob', 'sandwichless']\n",
    "# [0, 1, 0, 0, 0, 0, 0, 1, 1, 0]\n",
    "['drop', 'sandwich', 'said', 'sandwich', 'polic']\n",
    "# [1, 0, 0, 0, 1, 1, 2, 0, 0, 0]\n",
    "```\n",
    "\n",
    "It actually represents the position in which the token is located in the main vocabulary list and increases the count if the token is repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TFIDF`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFIDF** has two terms\n",
    "- **TF** Term Frequency\n",
    "- **IDF** Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Term Frequency`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$TF_{word,document} = \\frac{\\#\\_of\\_times\\_word\\_appears\\_in\\_document}{total\\_\\#\\_of\\_words\\_in\\_document}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "['jeff', 'stole', 'octopus', 'sandwich']\n",
    "[0, 0, 1/4, 1/4, 0, 0, 1/4, 0, 0, 1/4]\n",
    "\n",
    "['help', 'sob', 'sandwichless']\n",
    "[0, 1/3, 0, 0, 0, 0, 0, 1/3, 1/3, 0]\n",
    "\n",
    "['drop', 'sandwich', 'said', 'sandwich', 'polic']\n",
    "[1/5, 0, 0, 0, 1/5, 1/5, 2/5, 0, 0, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning, in our first **TF** list, the token `jeff` appears $1$ time in the list but a total of $4$ times in our corpus, hence we see $1/4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Document Frequency`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$DF_{word} = \\frac{\\#\\_of\\_documents\\_containing\\_word}{total\\_\\#\\_of\\_documents}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary:\n",
    "\n",
    "```python\n",
    "['drop', 'help', 'jeff', 'octopus', 'polic', 'said', 'sandwich', 'sandwichless', 'sobbed', 'stole']\n",
    "```\n",
    "\n",
    "Document frequency for each word:\n",
    "\n",
    "```python\n",
    "[1/3, 1/3, 1/3, 1/3, 1/3, 1/3, 2/3, 1/3, 1/3, 1/3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Inverse Document Frequency`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$IDF_{word}=log\\left(\\frac{total\\_\\#\\_of\\_documents}{\\#\\_of\\_documents\\_containing\\_word}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary:\n",
    "\n",
    "```python\n",
    "['drop', 'help', 'jeff', 'octopus', 'polic', 'said', 'sandwich', 'sandwichless', 'sobbed', 'stole']\n",
    "```\n",
    "\n",
    "IDF for each word:\n",
    "```python\n",
    "[1.099, 1.099, 1.099, 1.099, 1.099, 1.099, 0.405, 1.099, 1.099, 1.099]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TFIDF`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary:\n",
    "\n",
    "```python\n",
    "['drop', 'help', 'jeff', 'octopus', 'polic', 'said', 'sandwich', 'sandwichless', 'sobbed', 'stole']\n",
    "```\n",
    "\n",
    "$TF * IDF$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "['jeff', 'stole', 'octopus', 'sandwich']\n",
    "[0, 0, 0.275, 0.275, 0, 0, 0.101, 0, 0, 0.275]\n",
    "\n",
    "['help', 'sob', 'sandwichless']\n",
    "[0, 0.366, 0, 0, 0, 0, 0, 0.366, 0.366, 0]\n",
    "\n",
    "['drop', 'sandwich', 'said', 'sandwich', 'polic']\n",
    "[0.22, 0, 0, 0, 0.22, 0.22, 0.162, 0, 0, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Conclusion`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have turned our *DOCUMENTS* into *VECTORS*, we can put them into whatever machine learning algorithm we want!  \n",
    "We can use whatever kind of similarity measure we please!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.08115802],\n",
       "       [0.08115802, 1.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([[0, 0, 0.275, 0.275, 0, 0, 0.101, 0, 0, 0.275],  [0.22, 0, 0, 0, 0.22, 0.22, 0.162, 0, 0, 0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass two vectors to the function `cosine_similarity()`, and here we have passed the *first vector* that we got from **TFIDF** of the first document and the *second vector* is the **TFIDF** we got from the third document.  \n",
    "\n",
    "Below, we'll take a look at the *cosine similarity* of the second vector with the third vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([[0, 0.366, 0, 0, 0, 0, 0, 0.366, 0.366, 0],  [0.22, 0, 0, 0, 0.22, 0.22, 0.162, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "[next]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b4484ae063bf9ef0d5115063857dd0e382a2c73ed49a11a000ca56af03302b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
